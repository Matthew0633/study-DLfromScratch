{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 PTB Dataset (Penn Treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "Downloading ptb.valid.txt ... \n",
      "Done\n",
      "Downloading ptb.test.txt ... \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# PTB 데이터 다운로드 코드\n",
    "\n",
    "# # coding: utf-8\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append('..')\n",
    "# try:\n",
    "#     import urllib.request\n",
    "# except ImportError:\n",
    "#     raise ImportError('Use Python3!')\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "# key_file = {\n",
    "#     'train':'ptb.train.txt',\n",
    "#     'test':'ptb.test.txt',\n",
    "#     'valid':'ptb.valid.txt'\n",
    "# }\n",
    "# save_file = {\n",
    "#     'train':'ptb.train.npy',\n",
    "#     'test':'ptb.test.npy',\n",
    "#     'valid':'ptb.valid.npy'\n",
    "# }\n",
    "# vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "# dataset_dir = os.path.dirname(os.path.abspath(__name__))\n",
    "\n",
    "\n",
    "# def _download(file_name):\n",
    "#     file_path = dataset_dir + '/' + file_name\n",
    "#     if os.path.exists(file_path):\n",
    "#         return\n",
    "\n",
    "#     print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "#     try:\n",
    "#         urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "#     except urllib.error.URLError:\n",
    "#         import ssl\n",
    "#         ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#         urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "#     print('Done')\n",
    "\n",
    "\n",
    "# def load_vocab():\n",
    "#     vocab_path = dataset_dir + '/' + vocab_file\n",
    "\n",
    "#     if os.path.exists(vocab_path):\n",
    "#         with open(vocab_path, 'rb') as f:\n",
    "#             word_to_id, id_to_word = pickle.load(f)\n",
    "#         return word_to_id, id_to_word\n",
    "\n",
    "#     word_to_id = {}\n",
    "#     id_to_word = {}\n",
    "#     data_type = 'train'\n",
    "#     file_name = key_file[data_type]\n",
    "#     file_path = dataset_dir + '/' + file_name\n",
    "\n",
    "#     _download(file_name)\n",
    "\n",
    "#     words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "#     for i, word in enumerate(words):\n",
    "#         if word not in word_to_id:\n",
    "#             tmp_id = len(word_to_id)\n",
    "#             word_to_id[word] = tmp_id\n",
    "#             id_to_word[tmp_id] = word\n",
    "\n",
    "#     with open(vocab_path, 'wb') as f:\n",
    "#         pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "#     return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "# def load_data(data_type='train'):\n",
    "#     '''\n",
    "#         :param data_type: 데이터 유형: 'train' or 'test' or 'valid (val)'\n",
    "#         :return:\n",
    "#     '''\n",
    "#     if data_type == 'val': data_type = 'valid'\n",
    "#     save_path = dataset_dir + '/' + save_file[data_type]\n",
    "\n",
    "#     word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "#     if os.path.exists(save_path):\n",
    "#         corpus = np.load(save_path)\n",
    "#         return corpus, word_to_id, id_to_word\n",
    "\n",
    "#     file_name = key_file[data_type]\n",
    "#     file_path = dataset_dir + '/' + file_name\n",
    "#     _download(file_name)\n",
    "\n",
    "#     words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "#     corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "#     np.save(save_path, corpus)\n",
    "#     return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     for data_type in ('train', 'val', 'test'):\n",
    "#         load_data(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "# ptb data 특징 \n",
    "# 코드 사진 - load 및 sample convert\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "corpus, word_to_id, id_to_word = load_data('train')\n",
    "\n",
    "print('말뭉치 크기:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.5 PTB 데이터셋 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산 ...\n",
      "PPMI 계산 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasonpark\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:133: RuntimeWarning: overflow encountered in long_scalars\n",
      "C:\\Users\\jasonpark\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:133: RuntimeWarning: invalid value encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n",
      "12.0% 완료\n",
      "13.0% 완료\n",
      "14.0% 완료\n",
      "15.0% 완료\n",
      "16.0% 완료\n",
      "17.0% 완료\n",
      "18.0% 완료\n",
      "19.0% 완료\n",
      "20.0% 완료\n",
      "21.0% 완료\n",
      "22.0% 완료\n",
      "23.0% 완료\n",
      "24.0% 완료\n",
      "25.0% 완료\n",
      "26.0% 완료\n",
      "27.0% 완료\n",
      "28.0% 완료\n",
      "29.0% 완료\n",
      "30.0% 완료\n",
      "31.0% 완료\n",
      "32.0% 완료\n",
      "33.0% 완료\n",
      "34.0% 완료\n",
      "35.0% 완료\n",
      "36.0% 완료\n",
      "37.0% 완료\n",
      "38.0% 완료\n",
      "39.0% 완료\n",
      "40.0% 완료\n",
      "41.0% 완료\n",
      "42.0% 완료\n",
      "43.0% 완료\n",
      "44.0% 완료\n",
      "45.0% 완료\n",
      "46.0% 완료\n",
      "47.0% 완료\n",
      "48.0% 완료\n",
      "49.0% 완료\n",
      "50.0% 완료\n",
      "51.0% 완료\n",
      "52.0% 완료\n",
      "53.0% 완료\n",
      "54.0% 완료\n",
      "55.0% 완료\n",
      "56.0% 완료\n",
      "57.0% 완료\n",
      "58.0% 완료\n",
      "59.0% 완료\n",
      "60.0% 완료\n",
      "61.0% 완료\n",
      "62.0% 완료\n",
      "63.0% 완료\n",
      "64.0% 완료\n",
      "65.0% 완료\n",
      "66.0% 완료\n",
      "67.0% 완료\n",
      "68.0% 완료\n",
      "69.0% 완료\n",
      "70.0% 완료\n",
      "71.0% 완료\n",
      "72.0% 완료\n",
      "73.0% 완료\n",
      "74.0% 완료\n",
      "75.0% 완료\n",
      "76.0% 완료\n",
      "77.0% 완료\n",
      "78.0% 완료\n",
      "79.0% 완료\n",
      "80.0% 완료\n",
      "81.0% 완료\n",
      "82.0% 완료\n",
      "83.0% 완료\n",
      "84.0% 완료\n",
      "85.0% 완료\n",
      "86.0% 완료\n",
      "87.0% 완료\n",
      "88.0% 완료\n",
      "89.0% 완료\n",
      "90.0% 완료\n",
      "91.0% 완료\n",
      "92.0% 완료\n",
      "93.0% 완료\n",
      "94.0% 완료\n",
      "95.0% 완료\n",
      "96.0% 완료\n",
      "97.0% 완료\n",
      "98.0% 완료\n",
      "99.0% 완료\n",
      "100.0% 완료\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.6855840682983398\n",
      " we: 0.6377156376838684\n",
      " anybody: 0.5644021034240723\n",
      " something: 0.5535581707954407\n",
      " do: 0.516314685344696\n",
      "\n",
      "[query] year\n",
      " month: 0.710936963558197\n",
      " last: 0.6481241583824158\n",
      " earlier: 0.6403515934944153\n",
      " quarter: 0.6317357420921326\n",
      " next: 0.5938366055488586\n",
      "\n",
      "[query] car\n",
      " auto: 0.630263090133667\n",
      " luxury: 0.5402191877365112\n",
      " lexus: 0.5062943696975708\n",
      " domestic: 0.4924314320087433\n",
      " cars: 0.4792626202106476\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7830018401145935\n",
      " nissan: 0.6958362460136414\n",
      " motors: 0.6710101366043091\n",
      " lexus: 0.6244442462921143\n",
      " honda: 0.6114982962608337\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드 설명, 결과 해석\n",
    "# truncated SVD 설명 - 수식 그림 및 개념, sklearn\n",
    "# 2.5 정리\n",
    "\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산 ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('PPMI 계산 ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (빠르다!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (느리다)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI(점별 상호정보량) 생성\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 추론기반 기법과 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.76220207  2.02851122  0.72500847]]\n"
     ]
    }
   ],
   "source": [
    "# 통계기반기법 문제점 vs 추론(신경망) 장점\n",
    "# 추론기반기법 개념 116p.\n",
    "# 3.1.3 신경망 단어처리 (ohe) 그림 117,118, 119p. 그림 및 matmul\n",
    "# 120p. 그림, 코드\n",
    "import numpy as np\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_kernel",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
